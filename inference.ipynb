{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from diffusers import FluxPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTYPE = torch.bfloat16\n",
    "\n",
    "pipe = FluxPipeline.from_pretrained(\"black-forest-labs/FLUX.1-dev\", torch_dtype=DTYPE)\n",
    "pipe.to(\"cuda\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "prompt = \"A high-impact Telegram post with the text 'ATTENTION!': the background is a vibrant and intense gradient of red and orange, with a subtle radial burst effect emanating from the center. The word 'ATTENTION!' is placed prominently in the center in a bold, sans-serif font with a metallic finish, slightly tilted for added dynamism. Surrounding the text, there are subtle, glowing lines and digital glitch effects, creating a sense of urgency and importance. The overall style is modern and eye-catching, perfect for grabbing attention on social media.\"\n",
    "\n",
    "out = pipe(\n",
    "    prompt=prompt,\n",
    "    guidance_scale=3.5,\n",
    "    height=1024,\n",
    "    width=1024,\n",
    "    num_inference_steps=50,\n",
    "\tgenerator=torch.Generator(device='cuda').manual_seed(0)\n",
    ").images[0]\n",
    "\n",
    "plt.figure(figsize=(8, 8), dpi=300)\n",
    "plt.imshow(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import (\n",
    "    AutoencoderKL,\n",
    "    FlowMatchEulerDiscreteScheduler,\n",
    "    FluxPipeline,\n",
    "    FluxTransformer2DModel,\n",
    ")\n",
    "from transformers import CLIPTokenizer, PretrainedConfig, T5TokenizerFast, CLIPTextModel, T5EncoderModel\n",
    "from reflow.flux_utils import encode_imgs, decode_imgs, get_noise, get_schedule\n",
    "\n",
    "def get_models(pretrained_model_name_or_path):\n",
    "    scheduler = FlowMatchEulerDiscreteScheduler.from_pretrained(\n",
    "        pretrained_model_name_or_path,\n",
    "        subfolder=\"scheduler\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    )\n",
    "    tokenizer_one = CLIPTokenizer.from_pretrained(\n",
    "        pretrained_model_name_or_path,\n",
    "        subfolder=\"tokenizer\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    )\n",
    "    tokenizer_two = T5TokenizerFast.from_pretrained(\n",
    "        pretrained_model_name_or_path,\n",
    "        subfolder=\"tokenizer_2\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    )\n",
    "    text_encoder_one = CLIPTextModel.from_pretrained(\n",
    "        pretrained_model_name_or_path,\n",
    "        subfolder=\"text_encoder\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    )\n",
    "    text_encoder_two = T5EncoderModel.from_pretrained(\n",
    "        pretrained_model_name_or_path,\n",
    "        subfolder=\"text_encoder_2\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    )\n",
    "    vae = AutoencoderKL.from_pretrained(\n",
    "        pretrained_model_name_or_path,\n",
    "        subfolder=\"vae\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    )\n",
    "    transformer = FluxTransformer2DModel.from_pretrained(\n",
    "        pretrained_model_name_or_path,\n",
    "        subfolder=\"transformer\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    )\n",
    "\n",
    "    transformer.requires_grad_(False)\n",
    "    vae.requires_grad_(False)\n",
    "    text_encoder_one.requires_grad_(False)\n",
    "    text_encoder_two.requires_grad_(False)\n",
    "\n",
    "    return (\n",
    "        scheduler,\n",
    "        tokenizer_one,\n",
    "        tokenizer_two,\n",
    "        text_encoder_one,\n",
    "        text_encoder_two,\n",
    "        vae,\n",
    "        transformer\n",
    "    )\n",
    "\n",
    "# enforce text encoder to be fp32\n",
    "(\n",
    "\tscheduler,\n",
    "\ttokenizer_one,\n",
    "\ttokenizer_two,\n",
    "\ttext_encoder_one,\n",
    "\ttext_encoder_two,\n",
    "\tvae,\n",
    "\ttransformer\n",
    ") = get_models(\"black-forest-labs/FLUX.1-dev\")\n",
    "\n",
    "pipeline = FluxPipeline(\n",
    "\tscheduler=scheduler,\n",
    "\ttokenizer=tokenizer_one,\n",
    "\ttext_encoder=text_encoder_one,\n",
    "\ttokenizer_2=tokenizer_two,\n",
    "\ttext_encoder_2=text_encoder_two,\n",
    "\tvae=vae,\n",
    "\ttransformer=transformer,\n",
    ").to(\"cuda\")\n",
    "\n",
    "for name, param in transformer.named_parameters():\n",
    "    print(name, param.shape, param.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, set_peft_model_state_dict, get_peft_model_state_dict\n",
    "\n",
    "# 思路，先初始化 lora，然后拿到 state dict，然后 set_peft_model_state_dict\n",
    "transformer_lora_config = LoraConfig(\n",
    "        r=128,\n",
    "        lora_alpha=128,\n",
    "        init_lora_weights=\"gaussian\", # also try \"default\"\n",
    "        target_modules=[\"to_k\", \n",
    "                        \"to_q\", \n",
    "                        \"to_v\", \n",
    "                        \"to_out.0\",\n",
    "                        \"add_k_proj\",\n",
    "                        \"add_q_proj\",\n",
    "                        \"add_v_proj\",\n",
    "                        \"to_add_out\",\n",
    "                        \"norm.linear\",\n",
    "                        \"proj_mlp\",\n",
    "                        \"proj_out\",\n",
    "                        \"ff.net.0.proj\",\n",
    "                        \"ff.net.2\",\n",
    "                        \"ff_context.net.0.proj\",\n",
    "                        \"ff_context.net.2\",\n",
    "                        \"norm1.linear\",\n",
    "                        \"norm1_context.linear\",\n",
    "                        \"norm.linear\",\n",
    "                        \"timestep_embedder.linear_1\",\n",
    "                        \"timestep_embedder.linear_2\",\n",
    "                        \"guidance_embedder.linear_1\",\n",
    "                        \"guidance_embedder.linear_2\",\n",
    "                        ],\n",
    "    )\n",
    "\n",
    "transformer.add_adapter(transformer_lora_config)\n",
    "\n",
    "state_dict = get_peft_model_state_dict(transformer)\n",
    "\n",
    "for name in state_dict:\n",
    "\tprint(name, state_dict[name].shape, state_dict[name].dtype, state_dict[name].device, state_dict[name].requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = get_peft_model_state_dict(transformer)\n",
    "\n",
    "for name, param in transformer.named_parameters():\n",
    "\tif param.requires_grad:\n",
    "\t\tprint(name, param.shape, param.dtype, param.requires_grad)\n",
    "\n",
    "# transformer_lora_parameters = list(filter(lambda p: p.requires_grad, transformer.parameters()))\n",
    "# for param in transformer_lora_parameters:\n",
    "# \tprint(param.shape, param.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自己写的 load lora  \n",
    "\n",
    "from safetensors import safe_open\n",
    "\n",
    "lora_state_dict = {}\n",
    "safetensors_path = \"/root/autodl-tmp/data/2rf_inference_t.safetensors\"\n",
    "with safe_open(safetensors_path, framework=\"pt\", device=\"cpu\") as f:\n",
    "    for key in f.keys():\n",
    "        lora_state_dict[key] = f.get_tensor(key)\n",
    "        print(key, lora_state_dict[key].shape, lora_state_dict[key].dtype, lora_state_dict[key].device, lora_state_dict[key].requires_grad)\n",
    "        \n",
    "# check all key has lora\n",
    "for name in lora_state_dict:\n",
    "\tassert \"lora\" in name, name\n",
    "\n",
    "# for name, param in transformer.named_parameters():\n",
    "#     # Find the matching LoRA weight key\n",
    "#     lora_A_key = f\"{name}.lora_A.weight\"\n",
    "#     lora_B_key = f\"{name}.lora_B.weight\"\n",
    "    \n",
    "#     if lora_A_key in lora_weights:\n",
    "#         print(f\"Setting LoRA A weights for {name}\")\n",
    "#         param.lora_A.weight.data = lora_weights[lora_A_key]\n",
    "    \n",
    "#     if lora_B_key in lora_weights:\n",
    "#         print(f\"Setting LoRA B weights for {name}\")\n",
    "#         param.lora_B.weight.data = lora_weights[lora_B_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.utils import convert_unet_state_dict_to_peft\n",
    "\n",
    "lora_state_dict = FluxPipeline.lora_state_dict(\"/root/autodl-tmp/data/2rf_inference_t.safetensors\")\n",
    "\n",
    "# for key in lora_state_dict:\n",
    "# \tprint(key, lora_state_dict[key].shape, lora_state_dict[key].dtype, lora_state_dict[key].device, lora_state_dict[key].requires_grad)\n",
    "\n",
    "transformer_state_dict = {\n",
    "\tf'{k.replace(\"transformer.\", \"\")}': v for k, v in lora_state_dict.items() if k.startswith(\"transformer.\")\n",
    "}\n",
    "\n",
    "transformer_state_dict = convert_unet_state_dict_to_peft(transformer_state_dict)\n",
    "\n",
    "incompatible_keys = set_peft_model_state_dict(transformer, transformer_state_dict, adapter_name=\"default\")\n",
    "\n",
    "if incompatible_keys is not None:\n",
    "\t# check only for unexpected keys\n",
    "\tunexpected_keys = getattr(incompatible_keys, \"unexpected_keys\", None)\n",
    "\tif unexpected_keys:\n",
    "\t\tprint(\n",
    "\t\t\tf\"Loading adapter weights from state_dict led to unexpected keys not found in the model: \"\n",
    "\t\t\tf\" {unexpected_keys}. \"\n",
    "\t\t)\n",
    "\n",
    "print(incompatible_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = get_peft_model_state_dict(transformer)\n",
    "\n",
    "# for name in state_dict:\n",
    "# \tprint(name, state_dict[name].shape, state_dict[name].dtype, state_dict[name].device, state_dict[name].requires_grad)\n",
    "\n",
    "for name, param in transformer.named_parameters():\n",
    "\tif param.requires_grad:\n",
    "\t\tprint(name, param.shape, param.dtype, param.requires_grad, param.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def sample(prompt, height=1024, width=1024, guidance_scale=3.5):\n",
    "\t(\n",
    "\t\tprompt_embeds,         # save\n",
    "\t\tpooled_prompt_embeds,  # save\n",
    "\t\ttext_ids,\n",
    "\t) = pipeline.encode_prompt(\n",
    "\t\tprompt=prompt,\n",
    "\t\tprompt_2=prompt,\n",
    "\t\tdevice=pipeline.device,\n",
    "\t\tmax_sequence_length=512,\n",
    "\t)\n",
    "\n",
    "\tprompt_embeds = prompt_embeds.to(torch.bfloat16)\n",
    "\tpooled_prompt_embeds = pooled_prompt_embeds.to(torch.bfloat16)\n",
    "\n",
    "\tnoise = get_noise(  # save, shape [num_samples, 16, height // 8, width // 8]\n",
    "\t\tnum_samples=1,\n",
    "\t\theight=height,\n",
    "\t\twidth=width,\n",
    "\t\tdevice=\"cuda\",\n",
    "\t\tdtype=torch.bfloat16,\n",
    "\t\tseed=0,\n",
    "\t)\n",
    "\n",
    "\tlatent_image_ids = FluxPipeline._prepare_latent_image_ids(\n",
    "\t\tnoise.shape[0],\n",
    "\t\tnoise.shape[2],\n",
    "\t\tnoise.shape[3],\n",
    "\t\tnoise.device,\n",
    "\t\ttorch.bfloat16,\n",
    "\t)\n",
    "\n",
    "\tpacked_latents = FluxPipeline._pack_latents(\n",
    "\t\t# [num_samples, (height // 16 * width // 16), 16 * 2 * 2]\n",
    "\t\tnoise,\n",
    "\t\tbatch_size=noise.shape[0],\n",
    "\t\tnum_channels_latents=noise.shape[1],\n",
    "\t\theight=noise.shape[2],\n",
    "\t\twidth=noise.shape[3],\n",
    "\t)\n",
    "\n",
    "\ttimesteps = timesteps = get_schedule(  # shape: [num_inference_steps]\n",
    "\t\tnum_steps=50,\n",
    "\t\timage_seq_len=(height // 16) * (width // 16),  # vae // 8 and patchify // 2\n",
    "\t\tshift=True,  # Set True for Flux-dev, False for Flux-schnell\n",
    "\t)\n",
    "\n",
    "\twith pipeline.progress_bar(total=50) as progress_bar:\n",
    "\t\tfor t_curr, t_prev in zip(timesteps[:-1], timesteps[1:]):\n",
    "\t\t\tt_vec = torch.full((packed_latents.shape[0],), t_curr, dtype=packed_latents.dtype,\n",
    "\t\t\t\t\t\t\t\tdevice=packed_latents.device)\n",
    "\t\t\tguidance_vec = torch.full((packed_latents.shape[0],), guidance_scale,\n",
    "\t\t\t\t\t\t\t\t\t\tdevice=packed_latents.device,\n",
    "\t\t\t\t\t\t\t\t\t\tdtype=packed_latents.dtype)\n",
    "\t\t\tpred = transformer(\n",
    "\t\t\t\thidden_states=packed_latents,\n",
    "\t\t\t\ttimestep=t_vec,\n",
    "\t\t\t\tguidance=guidance_vec,\n",
    "\t\t\t\tpooled_projections=pooled_prompt_embeds,\n",
    "\t\t\t\tencoder_hidden_states=prompt_embeds,\n",
    "\t\t\t\ttxt_ids=text_ids,\n",
    "\t\t\t\timg_ids=latent_image_ids,\n",
    "\t\t\t\tjoint_attention_kwargs=None,\n",
    "\t\t\t\treturn_dict=pipeline,\n",
    "\t\t\t)[0]\n",
    "\t\t\tpacked_latents = packed_latents + (t_prev - t_curr) * pred\n",
    "\t\t\tprogress_bar.update()\n",
    "\n",
    "\tassert noise.shape[2] * 8 == height and noise.shape[3] * 8 == width\n",
    "\tassert pipeline.vae_scale_factor == 16\n",
    "\timg_latents = FluxPipeline._unpack_latents(  # save, shape [num_samples, 16, height//8, width//8]\n",
    "\t\tpacked_latents,\n",
    "\t\theight=height,\n",
    "\t\twidth=width,\n",
    "\t\tvae_scale_factor=pipeline.vae_scale_factor,\n",
    "\t)\n",
    "\n",
    "\timgs = decode_imgs(img_latents, vae, pipeline)[0]\n",
    "\n",
    "\treturn imgs\t\n",
    "\n",
    "prompt = \"A high-impact Telegram post with the text 'ATTENTION!': the background is a vibrant and intense gradient of red and orange, with a subtle radial burst effect emanating from the center. The word 'ATTENTION!' is placed prominently in the center in a bold, sans-serif font with a metallic finish, slightly tilted for added dynamism. Surrounding the text, there are subtle, glowing lines and digital glitch effects, creating a sense of urgency and importance. The overall style is modern and eye-catching, perfect for grabbing attention on social media.\"\n",
    "out = sample(prompt, height=1024, width=1024, guidance_scale=3.5)\n",
    "plt.figure(figsize=(8, 8), dpi=300)\n",
    "plt.imshow(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
