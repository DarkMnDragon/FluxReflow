{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from diffusers import FluxPipeline\n",
    "from torch import Tensor\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "DTYPE = torch.bfloat16\n",
    "pipe = FluxPipeline.from_pretrained(\"/root/autodl-tmp/data/FLUX-dev\", torch_dtype=DTYPE)\n",
    "pipe.load_lora_weights(\"/root/autodl-tmp/flux-lora-dreambooth\")\n",
    "pipe.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def decode_imgs(latents, pipeline):\n",
    "    imgs = (latents / pipeline.vae.config.scaling_factor) + pipeline.vae.config.shift_factor\n",
    "    imgs = pipeline.vae.decode(imgs)[0]\n",
    "    imgs = pipeline.image_processor.postprocess(imgs, output_type=\"pil\")\n",
    "    return imgs\n",
    "\n",
    "@torch.inference_mode()\n",
    "def encode_imgs(imgs, pipeline):\n",
    "    latents = pipeline.vae.encode(imgs).latent_dist.sample()\n",
    "    latents = (latents - pipeline.vae.config.shift_factor) * pipeline.vae.config.scaling_factor\n",
    "    latents = latents.to(dtype=torch.bfloat16)\n",
    "    return latents\n",
    "\n",
    "def get_noise(\n",
    "    num_samples: int,\n",
    "    height: int,\n",
    "    width: int,\n",
    "    device: torch.device,\n",
    "    dtype: torch.dtype,\n",
    "    seed: int,\n",
    "):\n",
    "    return torch.randn(  # [B, 16, H // 8, W // 8], latents after VAE\n",
    "        num_samples,\n",
    "        16,\n",
    "        2 * math.ceil(height / 16),\n",
    "        2 * math.ceil(width / 16),\n",
    "        device=device,\n",
    "        dtype=dtype,\n",
    "        generator=torch.Generator(device=device).manual_seed(seed),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_shift(mu: float, sigma: float, t: Tensor):\n",
    "    return math.exp(mu) / (math.exp(mu) + (1 / t - 1) ** sigma)\n",
    "\n",
    "def get_lin_function(\n",
    "    x1: float = 256, y1: float = 0.5, x2: float = 4096, y2: float = 1.15\n",
    "):\n",
    "    m = (y2 - y1) / (x2 - x1)\n",
    "    b = y1 - m * x1\n",
    "    return lambda x: m * x + b\n",
    "\n",
    "def get_schedule(\n",
    "    num_steps: int,\n",
    "    image_seq_len: int,\n",
    "    base_shift: float = 0.5,\n",
    "    max_shift: float = 1.15,\n",
    "    shift: bool = True,\n",
    ") -> list[float]:\n",
    "    timesteps = torch.linspace(1, 0, num_steps + 1)\n",
    "    if shift:\n",
    "        mu = get_lin_function(y1=base_shift, y2=max_shift)(image_seq_len)\n",
    "        timesteps = time_shift(mu, 1.0, timesteps)\n",
    "    return timesteps.tolist()\n",
    "\n",
    "timesteps = get_schedule( # shape: [num_inference_steps]\n",
    "            num_steps=100,\n",
    "            image_seq_len=(1024 // 16) * (1024 // 16), # vae_scale_factor = 16\n",
    "            shift=False,  # Set True for Flux-dev, False for Flux-schnell\n",
    "        )\n",
    "\n",
    "print(timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def forward_denoise(pipeline, timesteps, prompt, resolution=1024, guidance_scale=3.5, seed=0):\n",
    "    prompt_embeds, pooled_prompt_embeds, text_ids = pipeline.encode_prompt(\n",
    "        prompt=prompt, prompt_2=prompt, prompt_embeds=prompt_embeds, pooled_prompt_embeds=pooled_prompt_embeds)\n",
    "\n",
    "    noise = get_noise( # save, shape [num_samples, 16, resolution // 8, resolution // 8]\n",
    "        num_samples=1,\n",
    "        height=resolution,\n",
    "        width=resolution,\n",
    "        device=\"cuda\",\n",
    "        dtype=torch.bfloat16,\n",
    "        seed=seed,\n",
    "    )\n",
    "\n",
    "    latent_image_ids = FluxPipeline._prepare_latent_image_ids(\n",
    "        noise.shape[0],\n",
    "        noise.shape[2],\n",
    "        noise.shape[3],\n",
    "        noise.device,\n",
    "        torch.bfloat16,\n",
    "    )\n",
    "\n",
    "    packed_latents = FluxPipeline._pack_latents( # shape [num_samples, (resolution // 16 * resolution // 16), 16 * 2 * 2]\n",
    "        noise,\n",
    "        batch_size=noise.shape[0],\n",
    "        num_channels_latents=noise.shape[1],\n",
    "        height=noise.shape[2],\n",
    "        width=noise.shape[3],\n",
    "    )\n",
    "    \n",
    "    # Reversed denoising loop in latent space\n",
    "    with pipeline.progress_bar(total=len(timesteps)-1) as progress_bar:\n",
    "        for t_curr, t_prev in zip(timesteps[:-1], timesteps[1:]):\n",
    "            t_vec = torch.full((packed_latents.shape[0],), t_curr, dtype=packed_latents.dtype, device=packed_latents.device)\n",
    "            guidance_vec = torch.full((packed_latents.shape[0],), guidance_scale, device=packed_latents.device, dtype=packed_latents.dtype)\n",
    "            print(f\"time step: {t_vec[0]}\")\n",
    "            pred = pipeline.transformer(\n",
    "                    hidden_states=packed_latents, # shape: [batch_size, seq_len, num_channels_latents], e.g. [1, 4096, 64] for 1024x1024\n",
    "                    timestep=t_vec,        # range: [0, 1]\n",
    "                    guidance=guidance_vec, # scalar guidance values for each sample in the batch\n",
    "                    pooled_projections=pooled_prompt_embeds, # CLIP text embedding\n",
    "                    encoder_hidden_states=prompt_embeds,     # T5 text embedding\n",
    "                    txt_ids=text_ids,\n",
    "                    img_ids=latent_image_ids,\n",
    "                    joint_attention_kwargs=None,\n",
    "                    return_dict=pipeline,\n",
    "                )[0]\n",
    "            packed_latents = packed_latents + (t_prev - t_curr) * pred\n",
    "            progress_bar.update()\n",
    "    \n",
    "    img_latents = FluxPipeline._unpack_latents( # save, shape [num_samples, 16, resolution//8, resolution//8]\n",
    "            packed_latents,\n",
    "            height=1024,\n",
    "            width=1024,\n",
    "            vae_scale_factor=pipeline.vae_scale_factor,\n",
    "    )\n",
    "    return img_latents\n",
    "\n",
    "img_latents = forward_denoise(pipe, timesteps, \"a photo of sks dog running on the beach\", resolution=1024, guidance_scale=3.5, seed=0)\n",
    "\n",
    "out = decode_imgs(img_latents, pipe)[0]\n",
    "\n",
    "plt.figure(figsize=(8, 8), dpi=300)\n",
    "plt.imshow(out)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
