{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/root/miniconda3/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41a8f2d9f2a5454d8d72802e255f4e24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c81db31c4604375b983fb9355059ceb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FluxPipeline {\n",
       "  \"_class_name\": \"FluxPipeline\",\n",
       "  \"_diffusers_version\": \"0.31.0.dev0\",\n",
       "  \"_name_or_path\": \"/root/autodl-tmp/FLUX-dev\",\n",
       "  \"scheduler\": [\n",
       "    \"diffusers\",\n",
       "    \"FlowMatchEulerDiscreteScheduler\"\n",
       "  ],\n",
       "  \"text_encoder\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTextModel\"\n",
       "  ],\n",
       "  \"text_encoder_2\": [\n",
       "    \"transformers\",\n",
       "    \"T5EncoderModel\"\n",
       "  ],\n",
       "  \"tokenizer\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTokenizer\"\n",
       "  ],\n",
       "  \"tokenizer_2\": [\n",
       "    \"transformers\",\n",
       "    \"T5TokenizerFast\"\n",
       "  ],\n",
       "  \"transformer\": [\n",
       "    \"diffusers\",\n",
       "    \"FluxTransformer2DModel\"\n",
       "  ],\n",
       "  \"vae\": [\n",
       "    \"diffusers\",\n",
       "    \"AutoencoderKL\"\n",
       "  ]\n",
       "}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from diffusers import FluxPipeline\n",
    "from torch import Tensor\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "DTYPE = torch.bfloat16\n",
    "pipe = FluxPipeline.from_pretrained(\"/root/autodl-tmp/FLUX-dev\", torch_dtype=DTYPE)\n",
    "pipe.load_lora_weights(\"/root/autodl-tmp/lora_ckpt/reflow-dev-various/checkpoint-4500/pytorch_lora_weights.safetensors\")\n",
    "pipe.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def decode_imgs(latents, pipeline):\n",
    "    imgs = (latents / pipeline.vae.config.scaling_factor) + pipeline.vae.config.shift_factor\n",
    "    imgs = pipeline.vae.decode(imgs)[0]\n",
    "    imgs = pipeline.image_processor.postprocess(imgs, output_type=\"pil\")\n",
    "    return imgs\n",
    "\n",
    "@torch.inference_mode()\n",
    "def encode_imgs(imgs, pipeline):\n",
    "    latents = pipeline.vae.encode(imgs).latent_dist.sample()\n",
    "    latents = (latents - pipeline.vae.config.shift_factor) * pipeline.vae.config.scaling_factor\n",
    "    latents = latents.to(dtype=torch.bfloat16)\n",
    "    return latents\n",
    "\n",
    "def get_noise(\n",
    "    num_samples: int,\n",
    "    height: int,\n",
    "    width: int,\n",
    "    device: torch.device,\n",
    "    dtype: torch.dtype,\n",
    "    seed: int,\n",
    "):\n",
    "    return torch.randn(  # [B, 16, H // 8, W // 8], latents after VAE\n",
    "        num_samples,\n",
    "        16,\n",
    "        2 * math.ceil(height / 16),\n",
    "        2 * math.ceil(width / 16),\n",
    "        device=device,\n",
    "        dtype=dtype,\n",
    "        generator=torch.Generator(device=device).manual_seed(seed),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.9045307636260986, 0.7595109343528748, 0.5128440856933594, 0.0]\n"
     ]
    }
   ],
   "source": [
    "def time_shift(mu: float, sigma: float, t: Tensor):\n",
    "    return math.exp(mu) / (math.exp(mu) + (1 / t - 1) ** sigma)\n",
    "\n",
    "def get_lin_function(\n",
    "    x1: float = 256, y1: float = 0.5, x2: float = 4096, y2: float = 1.15\n",
    "):\n",
    "    m = (y2 - y1) / (x2 - x1)\n",
    "    b = y1 - m * x1\n",
    "    return lambda x: m * x + b\n",
    "\n",
    "def get_schedule(\n",
    "    num_steps: int,\n",
    "    image_seq_len: int,\n",
    "    base_shift: float = 0.5,\n",
    "    max_shift: float = 1.15,\n",
    "    shift: bool = True,\n",
    ") -> list[float]:\n",
    "    timesteps = torch.linspace(1, 0, num_steps + 1)\n",
    "    if shift:\n",
    "        mu = get_lin_function(y1=base_shift, y2=max_shift)(image_seq_len)\n",
    "        timesteps = time_shift(mu, 1.0, timesteps)\n",
    "    return timesteps.tolist()\n",
    "\n",
    "timesteps = get_schedule( # shape: [num_inference_steps]\n",
    "            num_steps=4,\n",
    "            image_seq_len=(1024 // 16) * (1024 // 16), # vae_scale_factor = 16\n",
    "            shift=True,  # Set True for Flux-dev, False for Flux-schnell\n",
    "        )\n",
    "\n",
    "print(timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['determination, achievement, and the beauty of nature. dramatic high quality']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe094de960944bb69ce897c41255bf43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time step: 1.0\n",
      "time step: 0.90625\n",
      "time step: 0.7578125\n",
      "time step: 0.51171875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['determination, achievement, and the beauty of nature. dramatic high quality']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5f70318401e4861b621461bfc4a34b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time step: 1.0\n",
      "time step: 0.95703125\n",
      "time step: 0.90625\n",
      "time step: 0.83984375\n",
      "time step: 0.7578125\n",
      "time step: 0.65625\n",
      "time step: 0.51171875\n",
      "time step: 0.310546875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['determination, achievement, and the beauty of nature. dramatic high quality']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4274307fbb8840f2a89b256011fcdd42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time step: 1.0\n",
      "time step: 0.98046875\n",
      "time step: 0.95703125\n",
      "time step: 0.93359375\n",
      "time step: 0.90625\n",
      "time step: 0.875\n",
      "time step: 0.83984375\n",
      "time step: 0.80078125\n",
      "time step: 0.7578125\n",
      "time step: 0.7109375\n",
      "time step: 0.65625\n",
      "time step: 0.58984375\n",
      "time step: 0.51171875\n",
      "time step: 0.421875\n",
      "time step: 0.310546875\n",
      "time step: 0.173828125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['determination, achievement, and the beauty of nature. dramatic high quality']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3cc059369834b7489c2962c01817feb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time step: 1.0\n",
      "time step: 0.98828125\n",
      "time step: 0.97265625\n",
      "time step: 0.95703125\n",
      "time step: 0.94140625\n",
      "time step: 0.921875\n",
      "time step: 0.90625\n",
      "time step: 0.8828125\n",
      "time step: 0.86328125\n",
      "time step: 0.83984375\n",
      "time step: 0.81640625\n",
      "time step: 0.7890625\n",
      "time step: 0.7578125\n",
      "time step: 0.7265625\n",
      "time step: 0.69140625\n",
      "time step: 0.65625\n",
      "time step: 0.61328125\n",
      "time step: 0.56640625\n",
      "time step: 0.51171875\n",
      "time step: 0.453125\n",
      "time step: 0.38671875\n",
      "time step: 0.310546875\n",
      "time step: 0.22265625\n",
      "time step: 0.12060546875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['determination, achievement, and the beauty of nature. dramatic high quality']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91b66cd832344673bccd513b9abde370",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time step: 1.0\n",
      "time step: 0.98828125\n",
      "time step: 0.98046875\n",
      "time step: 0.96875\n",
      "time step: 0.95703125\n",
      "time step: 0.9453125\n",
      "time step: 0.93359375\n",
      "time step: 0.91796875\n",
      "time step: 0.90625\n",
      "time step: 0.890625\n",
      "time step: 0.875\n",
      "time step: 0.859375\n",
      "time step: 0.83984375\n",
      "time step: 0.8203125\n",
      "time step: 0.80078125\n",
      "time step: 0.78125\n",
      "time step: 0.7578125\n",
      "time step: 0.734375\n",
      "time step: 0.7109375\n",
      "time step: 0.68359375\n",
      "time step: 0.65625\n",
      "time step: 0.625\n",
      "time step: 0.58984375\n",
      "time step: 0.5546875\n",
      "time step: 0.51171875\n",
      "time step: 0.46875\n",
      "time step: 0.421875\n",
      "time step: 0.369140625\n",
      "time step: 0.310546875\n",
      "time step: 0.24609375\n",
      "time step: 0.173828125\n",
      "time step: 0.09228515625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['determination, achievement, and the beauty of nature. dramatic high quality']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3e4eeee05634aad87d2b9350ccea370",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time step: 1.0\n",
      "time step: 0.9921875\n",
      "time step: 0.984375\n",
      "time step: 0.9765625\n",
      "time step: 0.96484375\n",
      "time step: 0.95703125\n",
      "time step: 0.9453125\n",
      "time step: 0.9375\n",
      "time step: 0.92578125\n",
      "time step: 0.9140625\n",
      "time step: 0.90625\n",
      "time step: 0.89453125\n",
      "time step: 0.87890625\n",
      "time step: 0.8671875\n",
      "time step: 0.85546875\n",
      "time step: 0.83984375\n",
      "time step: 0.82421875\n",
      "time step: 0.80859375\n",
      "time step: 0.79296875\n",
      "time step: 0.77734375\n",
      "time step: 0.7578125\n",
      "time step: 0.7421875\n",
      "time step: 0.72265625\n",
      "time step: 0.69921875\n",
      "time step: 0.6796875\n",
      "time step: 0.65625\n",
      "time step: 0.62890625\n",
      "time step: 0.6015625\n",
      "time step: 0.57421875\n",
      "time step: 0.546875\n",
      "time step: 0.51171875\n",
      "time step: 0.478515625\n",
      "time step: 0.44140625\n",
      "time step: 0.400390625\n",
      "time step: 0.357421875\n",
      "time step: 0.310546875\n",
      "time step: 0.259765625\n",
      "time step: 0.2041015625\n",
      "time step: 0.142578125\n",
      "time step: 0.07470703125\n"
     ]
    }
   ],
   "source": [
    "@torch.inference_mode()\n",
    "def forward_denoise(pipeline, num_steps, prompt, resolution=1024, guidance_scale=3.5, seed=0):\n",
    "    timesteps = get_schedule( # shape: [num_inference_steps]\n",
    "            num_steps=num_steps,\n",
    "            image_seq_len=(1024 // 16) * (1024 // 16), # vae_scale_factor = 16\n",
    "            shift=True,  # Set True for Flux-dev, False for Flux-schnell\n",
    "        )\n",
    "    \n",
    "    prompt_embeds, pooled_prompt_embeds, text_ids = pipeline.encode_prompt(prompt=prompt, prompt_2=prompt)\n",
    "\n",
    "    noise = get_noise( # save, shape [num_samples, 16, resolution // 8, resolution // 8]\n",
    "        num_samples=1,\n",
    "        height=resolution,\n",
    "        width=resolution,\n",
    "        device=\"cuda\",\n",
    "        dtype=torch.bfloat16,\n",
    "        seed=seed,\n",
    "    )\n",
    "\n",
    "    latent_image_ids = FluxPipeline._prepare_latent_image_ids(\n",
    "        noise.shape[0],\n",
    "        noise.shape[2],\n",
    "        noise.shape[3],\n",
    "        noise.device,\n",
    "        torch.bfloat16,\n",
    "    )\n",
    "\n",
    "    packed_latents = FluxPipeline._pack_latents( # shape [num_samples, (resolution // 16 * resolution // 16), 16 * 2 * 2]\n",
    "        noise,\n",
    "        batch_size=noise.shape[0],\n",
    "        num_channels_latents=noise.shape[1],\n",
    "        height=noise.shape[2],\n",
    "        width=noise.shape[3],\n",
    "    )\n",
    "    \n",
    "    # Reversed denoising loop in latent space\n",
    "    with pipeline.progress_bar(total=len(timesteps)-1) as progress_bar:\n",
    "        for t_curr, t_prev in zip(timesteps[:-1], timesteps[1:]):\n",
    "            t_vec = torch.full((packed_latents.shape[0],), t_curr, dtype=packed_latents.dtype, device=packed_latents.device)\n",
    "            guidance_vec = torch.full((packed_latents.shape[0],), guidance_scale, device=packed_latents.device, dtype=packed_latents.dtype)\n",
    "            print(f\"time step: {t_vec[0]}\")\n",
    "            pred = pipeline.transformer(\n",
    "                    hidden_states=packed_latents, # shape: [batch_size, seq_len, num_channels_latents], e.g. [1, 4096, 64] for 1024x1024\n",
    "                    timestep=t_vec,        # range: [0, 1]\n",
    "                    guidance=guidance_vec, # scalar guidance values for each sample in the batch\n",
    "                    pooled_projections=pooled_prompt_embeds, # CLIP text embedding\n",
    "                    encoder_hidden_states=prompt_embeds,     # T5 text embedding\n",
    "                    txt_ids=text_ids,\n",
    "                    img_ids=latent_image_ids,\n",
    "                    joint_attention_kwargs=None,\n",
    "                    return_dict=pipeline,\n",
    "                )[0]\n",
    "            packed_latents = packed_latents + (t_prev - t_curr) * pred\n",
    "            progress_bar.update()\n",
    "    \n",
    "    img_latents = FluxPipeline._unpack_latents( # save, shape [num_samples, 16, resolution//8, resolution//8]\n",
    "            packed_latents,\n",
    "            height=1024,\n",
    "            width=1024,\n",
    "            vae_scale_factor=pipeline.vae_scale_factor,\n",
    "    )\n",
    "    return img_latents\n",
    "\n",
    "# prompt = 'A water color painting of a lone hiker standing triumphantly on the summit of a snow-capped mountain. The hiker, wearing a red jacket and backpack, is silhouetted against a clear blue sky. The word \"ASCEND\" is painted in bold, blue letters at the base of the mountain, adding a powerful message to the image. The overall impression is one of determination, achievement, and the beauty of nature. Dramatic high quality'\n",
    "\n",
    "# prompt = 'A high resolution photo of Einstein, white background, photo-realistic, high-detail'\n",
    "\n",
    "# prompt = 'A vibrant, starry night sky illuminates a lively street café, with warm golden lights spilling from its windows. The café is nestled on a narrow cobblestone street, surrounded by rustic buildings with swirling, textured brushstrokes. Bold, dynamic colors—deep blues and glowing yellows—fill the scene. People are seated at small round tables, sipping coffee, and chatting. The atmosphere is cozy and inviting, yet full of movement and energy, capturing the timeless essence of a Van Gogh painting.'\n",
    "\n",
    "prompt = 'Jewelry design, a ring with bright rose-cut blue diamonds, surrounded by small lily-of-the-valley flower-shaped diamonds, golden stems form the ring of the ring. The center of the base is a beautiful rose gold, with a detachable black ring on both sides'\n",
    "\n",
    "steps_list = [4, 8, 16, 24, 32, 40]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10), dpi=300)\n",
    "\n",
    "for i, num_steps in enumerate(steps_list):\n",
    "    img_latents = forward_denoise(pipe, num_steps=num_steps, prompt=prompt, \n",
    "                                  resolution=1024, guidance_scale=3.5, seed=12)\n",
    "    out = decode_imgs(img_latents, pipe)[0]\n",
    "    \n",
    "    ax = axes[i // 3, i % 3]\n",
    "    ax.imshow(out)\n",
    "    ax.set_title(f\"{num_steps} Steps\")\n",
    "    ax.axis('off') \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
