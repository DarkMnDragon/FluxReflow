{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from diffusers import FluxPipeline\n",
    "from torch import Tensor\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "DTYPE = torch.bfloat16\n",
    "pipe = FluxPipeline.from_pretrained(\"black-forest-labs/FLUX.1-dev\", torch_dtype=DTYPE)\n",
    "# pipe.load_lora_weights(\"/root/autodl-tmp/data/2rf-full-lora.safetensors\")\n",
    "# pipe.load_lora_weights(\"/root/autodl-tmp/lora_ckpt/reflow-dev-various/checkpoint-4500/pytorch_lora_weights.safetensors\")\n",
    "# pipe.load_lora_weights(\"/root/autodl-tmp/data/Flux_Aquarell_Watercolor_v2.safetensors\", adapter_name=\"water\")\n",
    "# pipe.set_adapters([\"accelerate\", \"water\"], adapter_weights=[0.8, 0.8])\n",
    "# pipe.fuse_lora(adapter_names=[\"accelerate\", \"water\"], lora_scale=1.0)\n",
    "pipe.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def decode_imgs(latents, pipeline):\n",
    "    imgs = (latents / pipeline.vae.config.scaling_factor) + pipeline.vae.config.shift_factor\n",
    "    imgs = pipeline.vae.decode(imgs)[0]\n",
    "    imgs = pipeline.image_processor.postprocess(imgs, output_type=\"pil\")\n",
    "    return imgs\n",
    "\n",
    "@torch.inference_mode()\n",
    "def encode_imgs(imgs, pipeline):\n",
    "    latents = pipeline.vae.encode(imgs).latent_dist.sample()\n",
    "    latents = (latents - pipeline.vae.config.shift_factor) * pipeline.vae.config.scaling_factor\n",
    "    latents = latents.to(dtype=torch.bfloat16)\n",
    "    return latents\n",
    "\n",
    "def get_noise(\n",
    "    num_samples: int,\n",
    "    height: int,\n",
    "    width: int,\n",
    "    device: torch.device,\n",
    "    dtype: torch.dtype,\n",
    "    seed: int,\n",
    "):\n",
    "    return torch.randn(  # [B, 16, H // 8, W // 8], latents after VAE\n",
    "        num_samples,\n",
    "        16,\n",
    "        2 * math.ceil(height / 16),\n",
    "        2 * math.ceil(width / 16),\n",
    "        device=device,\n",
    "        dtype=dtype,\n",
    "        generator=torch.Generator(device=device).manual_seed(seed),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_shift(mu: float, sigma: float, t: Tensor):\n",
    "    return math.exp(mu) / (math.exp(mu) + (1 / t - 1) ** sigma)\n",
    "\n",
    "def get_lin_function(\n",
    "    x1: float = 256, y1: float = 0.5, x2: float = 4096, y2: float = 1.15\n",
    "):\n",
    "    m = (y2 - y1) / (x2 - x1)\n",
    "    b = y1 - m * x1\n",
    "    return lambda x: m * x + b\n",
    "\n",
    "def get_schedule(\n",
    "    num_steps: int,\n",
    "    image_seq_len: int,\n",
    "    base_shift: float = 0.5,\n",
    "    max_shift: float = 1.15,\n",
    "    shift: bool = True,\n",
    ") -> list[float]:\n",
    "    timesteps = torch.linspace(1, 0, num_steps + 1)\n",
    "    if shift:\n",
    "        mu = get_lin_function(y1=base_shift, y2=max_shift)(image_seq_len)\n",
    "        timesteps = time_shift(mu, 1.0, timesteps)\n",
    "    return timesteps.tolist()\n",
    "\n",
    "timesteps = get_schedule( # shape: [num_inference_steps]\n",
    "            num_steps=4,\n",
    "            image_seq_len=(1024 // 16) * (1024 // 16), # vae_scale_factor = 16\n",
    "            shift=True,  # Set True for Flux-dev, False for Flux-schnell\n",
    "        )\n",
    "\n",
    "print(timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def forward_denoise(pipeline, num_steps, prompt, resolution=512, guidance_scale=3.5, seed=0):\n",
    "    timesteps = get_schedule( # shape: [num_inference_steps]\n",
    "            num_steps=num_steps,\n",
    "            image_seq_len=(resolution // 16) * (resolution // 16), # vae_scale_factor = 16\n",
    "            shift=True,  # Set True for Flux-dev, False for Flux-schnell\n",
    "        )\n",
    "    \n",
    "    prompt_embeds, pooled_prompt_embeds, text_ids = pipeline.encode_prompt(prompt=prompt, prompt_2=prompt)\n",
    "\n",
    "    noise = get_noise( # save, shape [num_samples, 16, resolution // 8, resolution // 8]\n",
    "        num_samples=1,\n",
    "        height=resolution,\n",
    "        width=resolution,\n",
    "        device=\"cuda\",\n",
    "        dtype=torch.bfloat16,\n",
    "        seed=seed,\n",
    "    )\n",
    "\n",
    "    latent_image_ids = FluxPipeline._prepare_latent_image_ids(\n",
    "        noise.shape[0],\n",
    "        noise.shape[2],\n",
    "        noise.shape[3],\n",
    "        noise.device,\n",
    "        torch.bfloat16,\n",
    "    )\n",
    "\n",
    "    packed_latents = FluxPipeline._pack_latents( # shape [num_samples, (resolution // 16 * resolution // 16), 16 * 2 * 2]\n",
    "        noise,\n",
    "        batch_size=noise.shape[0],\n",
    "        num_channels_latents=noise.shape[1],\n",
    "        height=noise.shape[2],\n",
    "        width=noise.shape[3],\n",
    "    )\n",
    "    \n",
    "    # Reversed denoising loop in latent space\n",
    "    with pipeline.progress_bar(total=len(timesteps)-1) as progress_bar:\n",
    "        for t_curr, t_prev in zip(timesteps[:-1], timesteps[1:]):\n",
    "            t_vec = torch.full((packed_latents.shape[0],), t_curr, dtype=packed_latents.dtype, device=packed_latents.device)\n",
    "            guidance_vec = torch.full((packed_latents.shape[0],), guidance_scale, device=packed_latents.device, dtype=packed_latents.dtype)\n",
    "            print(f\"time step: {t_vec[0]}\")\n",
    "            pred = pipeline.transformer(\n",
    "                    hidden_states=packed_latents, # shape: [batch_size, seq_len, num_channels_latents], e.g. [1, 4096, 64] for 1024x1024\n",
    "                    timestep=t_vec,        # range: [0, 1]\n",
    "                    guidance=guidance_vec, # scalar guidance values for each sample in the batch\n",
    "                    pooled_projections=pooled_prompt_embeds, # CLIP text embedding\n",
    "                    encoder_hidden_states=prompt_embeds,     # T5 text embedding\n",
    "                    txt_ids=text_ids,\n",
    "                    img_ids=latent_image_ids,\n",
    "                    joint_attention_kwargs=None,\n",
    "                    return_dict=pipeline,\n",
    "                )[0]\n",
    "            packed_latents = packed_latents + (t_prev - t_curr) * pred\n",
    "            progress_bar.update()\n",
    "    \n",
    "    img_latents = FluxPipeline._unpack_latents( # save, shape [num_samples, 16, resolution//8, resolution//8]\n",
    "            packed_latents,\n",
    "            height=resolution,\n",
    "            width=resolution,\n",
    "            vae_scale_factor=pipeline.vae_scale_factor,\n",
    "    )\n",
    "    return img_latents\n",
    "\n",
    "# prompt = 'A water color painting of a lone hiker standing triumphantly on the summit of a snow-capped mountain. The hiker, wearing a red jacket and backpack, is silhouetted against a clear blue sky. The word \"ASCEND\" is painted in bold, blue letters at the base of the mountain, adding a powerful message to the image. The overall impression is one of determination, achievement, and the beauty of nature. Dramatic high quality'\n",
    "\n",
    "# prompt = 'A high resolution photo of a scientist, white background, photo-realistic, high-detail'\n",
    "\n",
    "# prompt = 'A vibrant, starry night sky illuminates a lively street café, with warm golden lights spilling from its windows. The café is nestled on a narrow cobblestone street, surrounded by rustic buildings with swirling, textured brushstrokes. Bold, dynamic colors—deep blues and glowing yellows—fill the scene. People are seated at small round tables, sipping coffee, and chatting. The atmosphere is cozy and inviting, yet full of movement and energy, capturing the timeless essence of a Van Gogh painting.'\n",
    "\n",
    "# prompt = 'Jewelry design, a ring with bright rose-cut blue diamonds, surrounded by small lily-of-the-valley flower-shaped diamonds, golden stems form the ring of the ring. The center of the base is a beautiful rose gold, with a detachable black ring on both sides'\n",
    "\n",
    "# prompt = 'An exquisite gothic queen vampiress with dark blue hair and crimson red eyes: Her sensuous white skin gleams in the atmospheric, dense fog, creating an epic and dramatic mood. This hyper-realistic portrait is filled with morbid beauty, from her gothicattire to the intense lighting that highlights every intricate detail. The scene combines glamour with dark, mysterious elements, blending fantasy and horror in a visually stunning way.'\n",
    "\n",
    "# prompt = \"A photo of a street performer juggling, with a sign that reads 'Tips Welcome' next to him.\"\n",
    "\n",
    "prompt = \"A billboard advertising a new movie, titled 'The Great Adventure', in bold cinematic style.\"\n",
    "\n",
    "\n",
    "steps_list = [1, 4, 8, 10, 16, 24]\n",
    "\n",
    "# steps_list = [40]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10), dpi=300)\n",
    "\n",
    "for i, num_steps in enumerate(steps_list):\n",
    "    img_latents = forward_denoise(pipe, num_steps=num_steps, prompt=prompt, \n",
    "                                  resolution=1024, guidance_scale=3.5, seed=100)\n",
    "    out = decode_imgs(img_latents, pipe)[0]\n",
    "    \n",
    "    ax = axes[i // 3, i % 3]\n",
    "    ax.imshow(out)\n",
    "    ax.set_title(f\"{num_steps} Steps\")\n",
    "    ax.axis('off') \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
